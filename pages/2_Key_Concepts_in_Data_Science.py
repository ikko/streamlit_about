
import streamlit as st
from itertools import batched

st.set_page_config(page_title="Key Concepts in Data Science", page_icon="ðŸ“„")

st.title("ðŸ“„ Key Concepts in Data Science")
summary = "This collection outlines various concepts in data science, including Poetry as a Python package tool, evaluation of classification through confusion matrices, and the functionality of Retrieval-Augmented Generation for enhancing language responses. It discusses ingest scoring in ETL pipelines for data quality, the branches of calculus, and the importance of stop words and TF-IDF in natural language processing. Additional topics cover unstructured data processing, Common Table Expressions in SQL for query management, the logical order of SQL clause execution, supply chain components, and perplexity in language models."
splitted = summary.split(". ")
double_sentences = ['.\n\n'.join('. '.join(x) for x in batched(splitted, 2))]
st.write('.\n\n'.join(double_sentences))
questions = [{"question": "What is the purpose and structure of a confusion matrix in classification models?", "answer": "A confusion matrix is a table used to evaluate the performance of classification models by comparing the predicted class labels with the actual labels. It typically consists of four components: True Positive (TP), True Negative (TN), False Positive (FP), and False Negative (FN). This structure allows for a clear understanding of how well a model is performing, indicating not only the correct predictions but also the types of errors being made, which is crucial in assessing and improving model accuracy."}, {"question": "What is Retrieval-Augmented Generation (RAG) and how does it enhance language generation?", "answer": "Retrieval-Augmented Generation (RAG) is a hybrid AI model that enhances language generation by retrieving relevant information from external sources before generating responses. Unlike standard language models that rely solely on pre-trained knowledge, RAG dynamically fetches real-time or external knowledge, making its responses more accurate and up-to-date. This combination of retrieval and generation improves the overall quality and relevance of AI responses."}, {"question": "What is ingest scoring in the context of an ETL pipeline, and why is it important?", "answer": "Ingest scoring in an ETL pipeline is the process of evaluating the quality, consistency, and reliability of incoming data during the data ingestion phase. It is crucial because it helps to ensure that only high-quality, relevant data is processed and loaded into the destination system. By assessing data quality, it allows for the early rejection of corrupt, incomplete, or duplicate data, compliance with business rules, and optimization of performance, which ultimately leads to better data integrity and decision-making."}, {"question": "What are the two main branches of calculus and their applications?", "answer": "The two main branches of calculus are Differentiation and Integration. Differentiation focuses on how a function's value changes in relation to its independent variable, using derivatives to identify the 'slope' or 'rate of change' at a specific point. Applications include finding maximum and minimum values for optimization, as well as calculating motion, speed, and acceleration. On the other hand, Integration deals with the accumulation of quantities, such as calculating the area under curves using integrals. This has applications in computing areas beneath curves, modeling quantity accumulation, and examining continuous probability distributions."}, {"question": "What are stop words and why is their removal important in natural language processing tasks according to Miklos's note?", "answer": "Stop words are common words like 'is', 'the', 'and' that frequently appear in a language but provide little semantic value in distinguishing between documents or understanding the meaning of text. Removing these words is important in natural language processing tasks as it reduces dataset size and eliminates noise that could negatively impact machine learning models. Focusing on meaningful words enhances the performance of models in tasks like sentiment analysis, document classification, and information retrieval."}, {"question": "What is the significance of TF-IDF in the context of natural language processing and how does IDF transform the way features are managed in datasets?", "answer": "TF-IDF (Term Frequency-Inverse Document Frequency) is a crucial statistical measure used in natural language processing (NLP) to evaluate the importance of a term in a document relative to a collection of documents, known as a corpus. It combines two components: Term Frequency (TF) which counts how frequently a term appears in a document, and Inverse Document Frequency (IDF), which assesses how unique a term is across all documents. By using IDF, common words that appear in many documents are down-weighted, allowing more meaningful and rare terms to gain higher weights, thereby improving the quality of features used for machine learning tasks. This transformation leads to better performance in downstream applications by focusing on terms that are more informative."}, {"question": "What is unstructured data and how is it processed?", "answer": "Unstructured data refers to information that does not have a predefined format or is not stored in traditional relational databases. This includes various types such as text, images, videos, JSON, XML, logs, and big data. Processing unstructured data requires specialized techniques involving parsing, storage, and analytics, often utilizing tools and libraries in Python designed for tasks like regular expressions for pattern matching, natural language processing (NLP), and handling large datasets using Apache Spark."}, {"question": "What are the key features of Common Table Expressions (CTEs) in SQL Server and how do they enhance query management?", "answer": "Common Table Expressions (CTEs) in SQL Server are defined as temporary result sets that simplify complex queries. Key features include their temporary and reusable nature, allowing multiple references within a single query without creating physical tables. They enhance readability and maintainability of queries, especially recursive ones, which are useful for handling hierarchical data, such as organizational structures. Additionally, CTEs eliminate the need for temporary tables, yet it is essential to be aware that they might affect performance compared to subqueries or temporary tables."}, {"question": "What is the significance of the logical order of clause interpretation in SQL and how are different clauses processed during a query execution?", "answer": "The logical order of clause interpretation in SQL is essential for understanding how queries are executed by the database engine, as it dictates the sequence in which clauses are processed, regardless of their written order in the query. The process begins with the `FROM` clause, which identifies the data sources and joins, followed by the `WHERE` clause that filters rows based on specified conditions. Next, the `GROUP BY` clause aggregates these rows, and the `HAVING` clause further filters these groups based on aggregate conditions. After grouping, the `SELECT` clause determines which columns or expressions to include in the results. The `DISTINCT` clause removes duplicates, the `ORDER BY` clause sorts the results, and finally, the optional `LIMIT/OFFSET` clause restricts the number of rows returned. This structured interpretation allows for efficient query execution and clearer understanding of the query's flow."}, {"question": "What is the definition of a supply chain and what are its key components and processes?", "answer": "A supply chain is a system where activities, processes, and participants work together to produce, move, and sell products or services. Key components of a supply chain include raw material procurement, production, logistics, warehousing, and distribution, encompassing the entire process from sourcing to the end consumer."}, {"question": "What is the concept of perplexity in relation to language models, and how is it mathematically defined?", "answer": "Perplexity is a statistical measurement used in Natural Language Processing (NLP) and Machine Learning to evaluate how well a probability model predicts a sample, particularly in language models (LMs). It quantifies the extent of surprise or uncertainty a model experiences upon encountering new text; a lower perplexity indicates more confident predictions by the model, while a higher perplexity signifies difficulty in predicting the next word. Mathematically, perplexity (PPL) is defined as PPL = 2^{H(P)}, where H(P) is the entropy of the probability distribution P, and it can also be computed using expected log probabilities for a sequence of words."}]

for qa in questions:
    with st.expander(f"ðŸ”¹ {qa['question']}"):
        answer = qa["answer"]
        st.markdown(''.join([f'- {s.strip()}.\n' for s in answer.split(".") if s]))
